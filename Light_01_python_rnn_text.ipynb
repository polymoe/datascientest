{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "deep_env",
      "language": "python",
      "name": "deep_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Light_01_python_rnn_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polymoe/datascientest/blob/main/Light_01_python_rnn_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "okKAGTR8rstq"
      },
      "source": [
        "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\" style=\"height:150px\">\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h1 style = \"text-align:center\" > Réseau de neurones récurrents </h1>\n",
        "\n",
        "\n",
        "Ce notebook est destiné à pratiquer les notions évoquées dans le premier exercice du module sur une machine plus adaptée.\n",
        "\n",
        "Si c'est la première fois que vous utilisez colab, n'hésitez pas à jeter un coup d'oeil sur ce [notebook](https://colab.research.google.com/drive/1jXEKOk3mRYBqFWoVwJ0ZpsRJCWj46Yxt?usp=sharing).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZFdyHrsbru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5006fff-1385-4e7d-ff81-fae879053d84"
      },
      "source": [
        "# Importation des données\n",
        "\n",
        "!wget https://train-exo.s3.eu-west-1.amazonaws.com/675/shakespeare.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-02 19:36:19--  https://train-exo.s3.eu-west-1.amazonaws.com/675/shakespeare.txt\n",
            "Resolving train-exo.s3.eu-west-1.amazonaws.com (train-exo.s3.eu-west-1.amazonaws.com)... 52.218.85.32\n",
            "Connecting to train-exo.s3.eu-west-1.amazonaws.com (train-exo.s3.eu-west-1.amazonaws.com)|52.218.85.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   1.06M  1.02MB/s    in 1.0s    \n",
            "\n",
            "2022-07-02 19:36:21 (1.02 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqTrz0Y6shnX"
      },
      "source": [
        "Un des interêts principaux de colab est la mise à disposition d'un GPU. Utiliser un GPU permet d'accelerer grandement l'execution et donc l'entrainement de modèle de deep learning. Pour configurer le GPU (processeur graphique), il suffit de cliquer sur Edit > Notebook settings et sélectionner GPU comme accélérateur matériel.\n",
        "* Exécuter la cellule suivante pour vérifier que le GPU soit bien activé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBu8hsgzsg5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0ee579-0e93-47c4-ecdb-dc56652887b4"
      },
      "source": [
        "import tensorflow as tf \n",
        "if tf.test.gpu_device_name(): \n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please change your hardware accelerator\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device:/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLeZmVaGwcRv"
      },
      "source": [
        "# Mise en Pratique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SGvulk4wYEI"
      },
      "source": [
        "* Ouvrir et lire le fichier shakespeare.txt avec un encoding 'utf-8'. Stocker le fichier dans la variable text.\n",
        "* Afficher les 250 premiers caractères.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OBzrx4hrsty"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "G1md5P6grstz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3ce061-986a-4ec5-f129-8c548a04bd01"
      },
      "source": [
        "#@title Solution\n",
        "with open(\"shakespeare.txt\", \"r\", encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "print(text[:250])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6FrJzr95NAu"
      },
      "source": [
        "* Stocker dans la variable **vocab** tous les caractères uniques du texte. Trier le résultat à l'aide de `sorted`.\n",
        "\n",
        "\n",
        "* Afficher le nombre de caractère unique.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
        "La commande `set` permet de retourner les éléments uniques d'une liste ou d'une chaine de caractère.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpWYtB9jrst2"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEmRrhjFf_EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a079439-aa3f-40bf-acc0-9c014d0e757d"
      },
      "source": [
        "#@title Solution\n",
        "vocab = sorted(set(text))\n",
        "print('{} uniques characters'.format(len(vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 uniques characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "rQUgGtAQrst4"
      },
      "source": [
        "### Convertir en index\n",
        "\n",
        "> Comme dans les tâches traditionnelles de text mining, les algorithmes ne traitent que des nombres et non des données textuelles. Il est alors nécessaire de convertir chaque caractère de notre corpus en nombre (ici, des indexes).\n",
        "\n",
        "* Créer un dictionnaire **char2idx**, dans le lequel vous allez stocker le **caractère en key** et **son index en value** :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbdm4OUJrst5"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "wy-NSFxhrst7"
      },
      "source": [
        "#@title Solution\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "OMHLL20brst7"
      },
      "source": [
        "* Exécuter la cellule suivante pour afficher les premiers éléments du dictionnaire **`char2idx`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WeERVofrst8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c21a6d7-0bfe-4ba1-c64c-c77f97657a41"
      },
      "source": [
        "import numpy as np\n",
        "print('{')\n",
        "for char, _ in zip(char2idx, range(10)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "SZ3VEgRtrst9"
      },
      "source": [
        "* Convertir dans la liste **`text_as_int`** tous les caractères du texte en index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AaE1j1Erst-"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "PVPOtg3Frst-"
      },
      "source": [
        "#@title Solution\n",
        "text_as_int = [char2idx[c] for c in text]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "B2YoZCHTrst_"
      },
      "source": [
        "## Génération du Dataset\n",
        "\n",
        "Nous allons implémenter dans la suite l'approche prédisant le caractère suivant de chaque élément en entrée (many to many). Nous allons choisir des séquences de caractère en entrée de taille 100.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "apAhax5zrsuA"
      },
      "source": [
        "* Importer **`tensorflow`** sous le nom **`tf`**.\n",
        "\n",
        "\n",
        "* Créer la variable **seq_length** = 100 représentant le nombre de caractère par séquence.\n",
        "\n",
        "\n",
        "* Définir un dataset **`char_dataset`** à l'aide de la fonction `from_tensor_slices` du jeu de données **`text_as_int`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezqAaDVbrsuB"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "rPbdNOLRrsuB"
      },
      "source": [
        "#@title Solution\n",
        "import tensorflow as tf\n",
        "seq_length = 100\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "CCJCP0versuD"
      },
      "source": [
        "> Nous allons maintenant appliquer une succession de transformation pour mettre notre jeu de données en forme :\n",
        ">\n",
        "><img src='https://datascientest.fr/train/assets/tensorflow_04_dataset.jpg' style='width:600px'>\n",
        "\n",
        "\n",
        "* À l'aide de la méthode `batch` appliquée à **`char_dataset`**, découper le jeu de données en batch de longueur **`seq_length + 1`**. Préciser en argument de la méthode **`drop_remainder = True`** indiquant que le dernier batch doit être supprimé dans le cas où il est incomplet. Stocker le résultat sous le nom **`sequences`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiNaSl1rsuE"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "C_aeaiFnrsuE"
      },
      "source": [
        "#@title Solution\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "yfupzX1prsuF"
      },
      "source": [
        "* Exécuter la cellule suivante pour afficher sous forme de caractères les 2 premiers éléments de **`char_dataset`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xACM5ul5rsuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c95bec7-ed33-477e-c389-ecb97ef0b34f"
      },
      "source": [
        "idx2char = np.array(vocab)\n",
        "for item in sequences.take(2):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "dVoKH7xgrsuG"
      },
      "source": [
        "* À l'aide de la méthode `map` de **`char_dataset`**, séparer les données d'entrées et les données cibles. Stocker le résultat sous le nom **`dataset`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzPHxMUfrsuG"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "SEeOl0fArsuH"
      },
      "source": [
        "#@title Solution\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "4QsfolBcrsuI"
      },
      "source": [
        "* Exécuter la cellule suivante pour afficher sous forme de caractères l'input et la target du premier élément de **dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "0gWQNcb3rsuJ"
      },
      "source": [
        "* Appliquer la méthode `shuffle` à **dataset** en précisant un **buffer_size** de 10000 pour mélanger le jeu de données.\n",
        "\n",
        "\n",
        "* Appliquer la méthode `batch` à **dataset** en précisant un **batch_size** de 64 pour séparer le jeu de données en batch. Préciser en argument de la méthode **drop_remainder = True** (ne pas prendre le dernier lot incomplet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFTsrN6WrsuJ"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "xP1cArFVrsuK"
      },
      "source": [
        "#@title Solution\n",
        "batch_size = 64\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "hFCzAdSDrsuK"
      },
      "source": [
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h2 style = \"text-align:center\" > Modélisation </h2>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        ">Dans la partie précédente, nous avons mis en forme notre jeu de données. Nous allons maintenant implémenter notre modèle RNN :\n",
        ">\n",
        "* Définir la variable **`vocab_size`** étant le nombre d'élément de notre dictionnaire.\n",
        "\n",
        "\n",
        "* Définir dans une fonction `build_model` avec comme argument **`batch_size`**:\n",
        "\n",
        "> * Instancier un modèle séquentiel **`model`** à l'aide du constructeur `Sequential` de **`tensorflow.keras`**.\n",
        ">\n",
        ">\n",
        "> * Définir une couche embedding `Embedding` en précisant une entrée de taille **`vocab_size`** et une sortie de taille 256. Par ailleurs, comme dans la suite nous allons utiliser **`stateful = True`** dans le RNN, il est nécessaire de préciser l'argument **`batch_input_shape = [batch_size, None]`**. \n",
        ">\n",
        ">\n",
        "> * Ajouter au **`model`** une couche `RNN` avec une cellule `GRUCell` de dimension 512 retournant une séquence et réutilisant le dernier état comme état initial $h_0$. \n",
        ">\n",
        ">\n",
        "> * Ajouter au **`model`** une couche `Dense` avec **vocab_size** neurones et une fonction d'activation `'softmax'`.\n",
        ">\n",
        ">\n",
        "> * Retourner le **`model`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdfir4pRrsuM"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "FNA2Htq3rsuM"
      },
      "source": [
        "#@title Solution\n",
        "from tensorflow.keras.layers import RNN, GRUCell, Dense, Embedding\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def build_model(batch_size):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(Embedding(vocab_size, 256,\n",
        "                         batch_input_shape=[batch_size, None]))\n",
        "\n",
        "    model.add(RNN(GRUCell(512), # Cell of RNN\n",
        "                return_sequences=True, # return a sequence\n",
        "                stateful=True))\n",
        "\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "BLPi6XhOrsuN"
      },
      "source": [
        "* Définir un modèle sous le nom **model** à l'aide de la fonction `build_model` avec un batch_size de 64.\n",
        "\n",
        "\n",
        "* Compiler le modèle à l'aide de sa méthode `compile` en précisant l'optimiseur `Adam` avec un learning rate de **1e-3** et la fonction de perte `sparse_categorical_crossentropy`.\n",
        "\n",
        "\n",
        "* Afficher le résumé du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwb5Q6q4rsuN"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "ReMvjpnZrsuN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf3f72e-3222-4a79-ce50-4b13310d72b6"
      },
      "source": [
        "#@title Solution\n",
        "# Create model\n",
        "model = build_model(64)\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy')\n",
        "# Summary\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " rnn (RNN)                   (64, None, 512)           1182720   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            33345     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,232,705\n",
            "Trainable params: 1,232,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "UiQHBWuvrsuO"
      },
      "source": [
        "* Entraîner le modèle sur les données **dataset** sur 30 epochs à l'aide de la méthode `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "-gJOFRHXrsuO"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "scrolled": true,
        "id": "R9QJmjDVrsuO",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bd8b47-8e4c-4117-bbd7-d7a2a75dd694"
      },
      "source": [
        "#@title Solution\n",
        "model.fit(dataset, epochs=30)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 21s 102ms/step - loss: 2.5975\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.9940\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.7453\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.6059\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 19s 105ms/step - loss: 1.5206\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.4643\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 20s 112ms/step - loss: 1.4237\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 22s 108ms/step - loss: 1.3913\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.3653\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.3442\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.3241\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 19s 104ms/step - loss: 1.3062\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.2909\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.2750\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.2612\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 19s 102ms/step - loss: 1.2463\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 19s 102ms/step - loss: 1.2341\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.2215\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.2091\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 19s 102ms/step - loss: 1.1977\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 19s 106ms/step - loss: 1.1851\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.1731\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.1623\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 18s 99ms/step - loss: 1.1515\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 19s 102ms/step - loss: 1.1400\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.1298\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.1189\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.1087\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 19s 101ms/step - loss: 1.0983\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 18s 100ms/step - loss: 1.0881\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f497043f690>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "1Unaj2HJrsuP"
      },
      "source": [
        "> Ici, le nombre de batch est fixé, il n'est alors pas possible de générer des textes avec des batchs autres que 64. Pour contourner cette limitation, c'est-à-dire traiter un unique texte, on peut utiliser l'astuce suivante :\n",
        ">\n",
        "> * Sauvegarder les poids de notre modèle à l'aide de la méthode `save_weights`.\n",
        ">\n",
        ">\n",
        "> * Recharger un nouveau modèle avec un batch_size de 1.\n",
        ">\n",
        ">\n",
        "> * Charger les poids de l'ancien modèle à l'aide de la méthode `load_weights`.\n",
        "\n",
        "\n",
        "* Sauvegarder les poids du modèle à l'aide de la méthode `save_weights` en précisant le chemin **\"model_rnn.h5\"**.\n",
        "\n",
        "\n",
        "* Créer un nouveau modèle sous le nom **`model`** à l'aide de la fonction `build_model` en précisant un batch_size de 1.\n",
        "\n",
        "\n",
        "* Charger les poids de notre modèle entraîné à l'aide de la méthode `load_weights` en précisant le chemin **\"model_rnn.h5\"**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y0xsZv9rsuP"
      },
      "source": [
        "## Insérez votre code ici\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "function": "solution",
        "question_id": 1,
        "id": "hSyu0K1UrsuP"
      },
      "source": [
        "#@title Solution\n",
        "# Save weights\n",
        "model.save_weights('model_rnn.h5')\n",
        "# Create a new model with a batch_size of 1.\n",
        "model = build_model(1)\n",
        "# Load weights\n",
        "model.load_weights('model_rnn.h5')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "eV5h6xjursuP"
      },
      "source": [
        "* Exécuter la cellule suivante pour générer un texte commençant par **\"ROMEO: \"**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_QSCBdFrsuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ab5f2d-8123-490f-c7db-1a851c6998f7"
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 500):\n",
        "    # Converting our start string to index (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    # Simulate a batch of 1 element\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    # List contains the text generated\n",
        "    text_generated = []\n",
        "    # Reset initial state\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        # Probability prediction\n",
        "        prediction = model(input_eval)\n",
        "        # Index prediction\n",
        "        index = tf.argmax(prediction, axis=-1).numpy()[0]\n",
        "        input_eval = tf.expand_dims([index[-1]], 0)\n",
        "        # Save letter in text_generated list\n",
        "        text_generated.append(idx2char[index[-1]])\n",
        "    # Return all sequence\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"ROMEO:\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "He is a word with you, and let them gave thee strike.\n",
            "\n",
            "GRUMIO:\n",
            "A boy, the matter, the conquest thousand men\n",
            "That thou art a word with the sacrament was the world to thee,\n",
            "That we will be the sun that the death of the world they are not so fair,\n",
            "So many thousand men are no lesser than the life\n",
            "To bear the senate was the matter, he shall not be so fair.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What is the stroke of it. A shadow lies me with the season which he has been broke,\n",
            "And so I think, that I may be contented:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "Me71LF02rsuQ"
      },
      "source": [
        "> Dans l'ensemble de l'exercice, vous avez formé un réseau de neurones pour **générer du texte**. Même si la longueur du texte que nous échantillonnons est assez petite, nous pouvons néanmoins remarquer certaines caractéristiques intéressantes du texte généré. Par exemple, le réseau apprend quelques mots de base comme «and», «of» «you» et «could» assez rapidement dans la formation. \n",
        ">\n",
        "> En outre, il apprend également quelques règles grammaticales de base: mettre des majuscules au premier mot de la phrase, finir une phrase par de la ponctuation, poser des questions fonctionne si les phrases commencent par des mots comme «what», «where». Le système n'est pas parfait mais étant donné que nous avons alimenté le réseau en caractères simples, il est remarquable que le réseau soit capable d'apprendre ces dépendances à long terme.\n",
        "\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<h2 style = \"text-align:center\" > Ce qu'il faut retenir </h2> \n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "### Quand utiliser les RNN ?\n",
        "\n",
        "> Les couches *récurrentes* sont utilisées lorsque les données d'entrées sont séquentielles et que la prédiction du prochain élément de la suite dépend fortement de la prédiction effectuée sur les éléments précédents. Typiquement, les cas où les couches récurrentes sont les plus utilisées sont:\n",
        ">\n",
        "> * Une suite de mots arrangés séquentiellement, c'est-à-dire un texte.\n",
        ">\n",
        ">\n",
        "> * Une suite de fréquences, c'est-à-dire un son ou une série temporelle.\n",
        ">\n",
        ">\n",
        "> * Une suite d'images, c'est-à-dire une vidéo.\n",
        "\n",
        "### Représentation vectoriel des caractères\n",
        "\n",
        "> L'entrée du modèle est alimentée par une séquence d'indice de caractère. Comme pour les variables catégorielles, il est nécessaire de les transformer sous forme d'un vecteur (get_dumnies, one hot, embedding ...) :\n",
        ">\n",
        "> * Couche `Embedding` (présenté dans le prochain exercice) :\n",
        ">\n",
        "> ```python\n",
        ">tf.keras.layers.Embedding(input_dim=vocab_size,       # l'indice maximal + 1 (taille du vocabulaire)\n",
        ">                           output_dim=embedding_dim)   # taille du vecteur en sortie \n",
        ">```\n",
        ">\n",
        ">\n",
        "> * Représentation `one_hot` : Il n'existe pas de layer **one hot** dans le package Keras. Pour ce faire, la fonction `Lambda` de **`tensorflow.keras.layers`** permet de créer un layer personnalisée avec comme argument la fonction à appliquer et la forme de sortie **`output_shape`**.\n",
        ">\n",
        ">```python\n",
        "layer_one_hot = Lambda(lambda x: tf.one_hot(x, depth), output_shape=[depth])\n",
        ">```\n",
        "\n",
        "\n",
        "### Entrée/sortie RNN :\n",
        "\n",
        "> Il existe différentes catégories d'entrée/sortie pour les RNN :\n",
        ">\n",
        "> * **Many to One** : Prédire une valeur de sortie $t_n$ en fonction de la séquence d'entrée $t_{n-1}$ ... $t_0$.\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/python-deeplnp-text-tm-many-to-one.png' style='width:600px'>\n",
        ">\n",
        ">\n",
        "> * **Many to Many** : Prédire une séquence [$t_n$ ... $t_1$] en fonction de la séquence d'entrée $t_{n-1}$ ... $t_0$.\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/python-deeplnp-text-tm-many-to-many.png' style='width:600px'>\n",
        ">\n",
        ">\n",
        "> * **Many to Many** : Prédire une séquence [$t_n$ ... $t_{i}$] en fonction de la séquence d'entrée $t_{i-1}$ ... $t_0$.\n",
        ">\n",
        "> <img src='https://assets-datascientest.s3-eu-west-1.amazonaws.com/notebooks/python-deeplnp-text-tm-many-to-many V2.png' style='width:600px'>\n",
        "\n",
        "\n",
        "### Cellule de calcul RNN\n",
        "\n",
        ">Il existe différentes cellules de calcul RNN (`SimpleRNNCell`, `LSTMCell` et `GRUCell`). Cette notion sera étudiée plus en profondeur dans les prochains exercices.\n",
        ">\n",
        "><img src='https://datascientest.fr/train/assets/tensorflow_04_rnn_gru_lstm.png' style='width:700px'>\n",
        ">\n",
        "><center><b>RNN Cell</b></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_oOL31r8GE5"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}